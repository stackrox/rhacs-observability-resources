apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    app: rhacs
  name: rhacs-data-plane-prometheus-rules
spec:
  groups:
    - name: rhacs-central
      rules:
        - alert: RHACSCentralScrapeFailed
          expr: |
            avg_over_time(up{pod=~"central-.*"}[10m]) < 0.5 and ON(pod) kube_pod_container_status_ready{container="central"} == 1
          for: 20m
          labels:
            severity: critical
          annotations:
            summary: "Prometheus unable to scrape metrics from target `{{ $labels.pod }}` in namespace `{{ $labels.namespace }}`."
            description: "During the last 10 minutes, only `{{ $value | humanizePercentage }}` of scrapes of target `{{ $labels.pod }}` in namespace `{{ $labels.namespace }}` were successful. This alert is raised when less than 50% of scrapes are successful."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-003-rhacs-instance-unavailable.md"
        - alert: RHACSCentralContainerDown
          expr: |
            avg_over_time(kube_pod_container_status_ready{container="central"}[10m]) < 0.5
          for: 20m
          labels:
            severity: critical
          annotations:
            summary: "Central container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` is down or in a CrashLoopBackOff status."
            description: "Central container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` has been down or in a CrashLoopBackOff status for at least 10 minutes."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-003-rhacs-instance-unavailable.md"
        - alert: RHACSCentralContainerFrequentlyRestarting
          expr: |
            increase(kube_pod_container_status_restarts_total{container="central"}[30m]) > 3
          labels:
            severity: critical
          annotations:
            summary: "Central container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` restarted more than 3 times."
            description: "Central container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` has restarted more than 3 times during the last 30 minutes."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-003-rhacs-instance-unavailable.md"
        - alert: RHACSCentralPostgresConnectionDown
          expr: avg_over_time(rox_central_postgres_connected{container="central"}[10m]) < 0.5
          for: 20m
          labels:
            severity: critical
          annotations:
            summary: "Central container `{{ $labels.pod }}/{{ $labels.container }}` database connection in namespace `{{ $labels.namespace }}` is down or is in a bad shape."
            description: "Central container `{{ $labels.pod }}/{{ $labels.container }}` database connection in namespace `{{ $labels.namespace }}` has been down or is in a bad shape for at least 10 minutes."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-010-rhacs-instance-db-unavailable.md"

    - name: rhacs-scanner
      rules:
        - alert: RHACSScannerScrapeFailed
          expr: |
            avg_over_time(up{pod=~"scanner-.*"}[10m]) < 0.5 and ON(pod) kube_pod_container_status_ready{pod=~"scanner.*", container=~"scanner|db"} == 1
          for: 20m
          labels:
            severity: critical
          annotations:
            summary: "Prometheus unable to scrape metrics from target `{{ $labels.pod }}` in namespace `{{ $labels.namespace }}`."
            description: "During the last 10 minutes, only `{{ $value | humanizePercentage }}` of scrapes of target `{{ $labels.pod }}` in namespace `{{ $labels.namespace }}` were successful. This alert is raised when less than 50% of scrapes are successful."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-003-rhacs-instance-unavailable.md"
        - alert: RHACSScannerContainerDown
          expr: |
            avg_over_time(kube_pod_container_status_ready{pod=~"scanner.*", container=~"scanner|db"}[10m]) < 0.5
          for: 20m
          labels:
            severity: critical
          annotations:
            summary: "Scanner container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` is down or in a CrashLoopBackOff status."
            description: "Scanner container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` has been down or in a CrashLoopBackOff status for at least 10 minutes."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-003-rhacs-instance-unavailable.md"
        - alert: RHACSScannerContainerFrequentlyRestarting
          expr: |
            increase(kube_pod_container_status_restarts_total{pod=~"scanner.*", container=~"scanner|db"}[30m]) > 3
          labels:
            severity: warning
          annotations:
            summary: "Scanner container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` restarted more than 3 times."
            description: "Scanner container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` has restarted more than 3 times during the last 30 minutes."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-003-rhacs-instance-unavailable.md"

    - name: rhacs-fleetshard
      rules:
        - alert: RHACSFleetshardOperatorContainerDown
          expr: |
            avg_over_time(kube_pod_container_status_ready{pod=~"rhacs-operator-controller-manager-.*"}[10m]) < 0.5
          for: 20m
          labels:
            severity: critical
          annotations:
            summary: "Fleetshard operator container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` is down or in a CrashLoopBackOff status."
            description: "Fleetshard operator container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` has been down or in a CrashLoopBackOff status for at least 10 minutes."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-011-rhacs-operator-unavailable.md"
        - alert: RHACSFleetshardOperatorContainerFrequentlyRestarting
          expr: |
            increase(kube_pod_container_status_restarts_total{pod=~"rhacs-operator-controller-manager-.*"}[30m]) > 3
          labels:
            severity: critical
          annotations:
            summary: "Fleetshard operator container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` restarted more than 3 times."
            description: "Fleetshard operator container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` has restarted more than 3 times during the last 30 minutes."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-011-rhacs-operator-unavailable.md"
        - alert: RHACSFleetshardSyncScrapeFailed
          expr: |
            (avg_over_time(up{pod=~"fleetshard-sync-.*"}[10m]) < 0.5 and ON(pod) kube_pod_container_status_ready{pod=~"fleetshard-sync-.*"} == 1) or absent(up{pod=~"fleetshard-sync-.*"})
          for: 20m
          labels:
            severity: critical
          annotations:
            summary: "Prometheus unable to scrape metrics from target `{{ $labels.pod }}` in namespace `{{ $labels.namespace }}`."
            description: "During the last 10 minutes, only `{{ $value | humanizePercentage }}` of scrapes of target `{{ $labels.pod }}` in namespace `{{ $labels.namespace }}` were successful. This alert is raised when less than 50% of scrapes are successful."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-005-fleetshard-sync-unavailable.md"
        - alert: RHACSFleetshardSyncContainerDown
          expr: |
            avg_over_time(kube_pod_container_status_ready{pod=~"fleetshard-sync-.*"}[10m]) < 0.5
          for: 20m
          labels:
            severity: critical
          annotations:
            summary: "Fleetshard synchronizer container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` is down or in a CrashLoopBackOff status."
            description: "Fleetshard synchronizer container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` has been down or in a CrashLoopBackOff status for at least 10 minutes."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-005-fleetshard-sync-unavailable.md"
        - alert: RHACSFleetshardSyncContainerFrequentlyRestarting
          expr: increase(kube_pod_container_status_restarts_total{pod=~"fleetshard-sync-.*"}[30m]) > 3
          labels:
            severity: critical
          annotations:
            summary: "Fleetshard synchronizer container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` restarted more than 3 times."
            description: "Fleetshard synchronizer container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` has restarted more than 3 times during the last 30 minutes."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-005-fleetshard-sync-unavailable.md"
        - alert: RHACSFleetshardSyncReconciliationErrors
          expr: |
            acs_fleetshard_central_errors_per_reconciliations:ratio_rate10m > 0.10
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Fleetshard synchronizer container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` failed to reconcile Central instances."
            description: "Fleetshard synchronizer container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` has a reconciliation error rate of {{ $value | humanizePercentage }} over the last 10 minutes."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-007-fleetshard-sync-reconciliation-error.md"
        - alert: RHACSFleetshardSyncFleetManagerRequestErrors
          expr: |
            acs_fleetshard_fleet_manager_errors_per_requests:ratio_rate10m > 0.01
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Fleetshard synchronizer container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` failed to reach fleet manager."
            description: "Fleetshard synchronizer container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` has a fleet manager request error rate of {{ $value | humanizePercentage }} over the last 10 minutes."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-007-fleetshard-sync-reconciliation-error.md"
        - alert: RHACSFleetshardSyncInvalidCentralCount
          expr: |
            sum(acs_fleetshard_total_centrals) <= 0
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "Fleetshard synchronizer manages `{{ $value }}` centrals."
            description: "Fleetshard synchronizer manages `{{ $value }}` centrals. The number of Centrals should always be larger than zero in a working system. If it drops to or below zero, fleetshard synchronizer is assumed to be in a failed state."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-007-fleetshard-sync-reconciliation-error.md"

    - name: tenant-resources
      rules:
        - expr: |
            sum(container_memory_working_set_bytes{namespace=~"rhacs-.{20}",container!="POD",container!=""}) by (namespace, container, pod)
          record: rhacs_tenants:namespace:pod:container:max_memory_usage_bytes
        - expr: |
            sum(container_spec_memory_limit_bytes{namespace=~"rhacs-.{20}",container!="POD",container!=""}) by (namespace, container, pod)
          record: rhacs_tenants:namespace:pod:container:memory_limit_bytes
        - expr: |
            rhacs_tenants:namespace:pod:container:max_memory_usage_bytes / rhacs_tenants:namespace:pod:container:memory_limit_bytes
          record: rhacs_tenants:namespace:pod:container:max_memory_usage_ratio
        - alert: RHACSTenantWorkloadMemoryUtilizationHigh
          expr: |
            rhacs_tenants:namespace:pod:container:max_memory_usage_ratio{container="central"} >= 0.85
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: tenant '{{ $labels.namespace }}' container '{{ $labels.container }}' in pod '{{ $labels.pod }}' is reaching its memory limit.
            description: tenant '{{ $labels.namespace }}' container '{{ $labels.container }}' in pod '{{ $labels.pod }}' reached {{ $value | humanizePercentage }} of its memory limit and is at risk of being OOM killed.
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-cloud-service/runbooks/-/blob/master/sops/dp-039-tenant-workload-memory-utilization-high.md"
        - alert: RHACSTenantWorkloadMemoryUtilizationCritical
          expr: |
            rhacs_tenants:namespace:pod:container:max_memory_usage_ratio{container="central"} >= 0.95
          for: 10m
          labels:
            severity: critical
          annotations:
            summary: tenant '{{ $labels.namespace }}' container '{{ $labels.container }}' in pod '{{ $labels.pod }}' is critically reaching its memory limit.
            description: tenant '{{ $labels.namespace }}' container '{{ $labels.container }}' in pod '{{ $labels.pod }}' reached {{ $value | humanizePercentage }} of its memory limit and is at high risk of being OOM killed.
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-cloud-service/runbooks/-/blob/master/sops/dp-039-tenant-workload-memory-utilization-high.md"

    - name: rhacs-operator
      rules:
        - expr: |
            sum (namespace_workload_pod:kube_pod_owner:relabel{namespace="rhacs"}
            * on (pod, namespace) group_left() kube_pod_labels{namespace="rhacs", label_app="rhacs-operator"})
            by (pod, namespace, workload)
          record: rhacs_operator:namespace:workload:pod:container
        - expr: |
            sum(container_memory_max_usage_bytes{namespace="rhacs",container!~"POD|"}) by (container, pod, namespace)
            * on (namespace, pod) group_left(workload) rhacs_operator:namespace:workload:pod:container
          record: rhacs_operator:namespace:workload:pod:container:max_memory_usage_bytes
        - expr: |
            sum(container_spec_memory_limit_bytes{namespace="rhacs",container!~"POD|"}) by (container, pod, namespace)
            * on (namespace, pod) group_left(workload) rhacs_operator:namespace:workload:pod:container
          record: rhacs_operator:namespace:workload:pod:container:memory_limit_bytes
        - expr: |
            sum(
              rhacs_operator:namespace:workload:pod:container:max_memory_usage_bytes
            / rhacs_operator:namespace:workload:pod:container:memory_limit_bytes)
            by (namespace, workload, container)
          record: rhacs_operator:namespace:workload:container:max_memory_usage_ratio
        - alert: RHACSOperatorMemoryUtilizationHigh
          expr: |
            rhacs_operator:namespace:workload:container:max_memory_usage_ratio > 0.7
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: The container '{{ $labels.container }}' in operator '{{ $labels.workload }}' is reaching its memory limit.
            description: The container '{{ $labels.container }}' in operator '{{ $labels.workload }}' reached {{ $value | humanizePercentage }} of its memory limit and is at risk of being OOM killed.
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-037-operator-memory-high.md"
        - alert: RHACSOperatorMemoryUtilizationCritical
          expr: |
            rhacs_operator:namespace:workload:container:max_memory_usage_ratio > 0.9
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: The container '{{ $labels.container }}' in operator '{{ $labels.workload }}' is critically reaching its memory limit.
            description: The container '{{ $labels.container }}' in operator '{{ $labels.workload }}' reached {{ $value | humanizePercentage }} of its memory limit and is at high risk of being OOM killed.
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-037-operator-memory-high.md"


    - name: rhacs-aws-quota
      rules:
        - alert: RHACSCentralDBClustersUtilizationHigh
          expr: |
            acs_fleetshard_central_db_clusters_used / acs_fleetshard_central_db_clusters_max >= 0.8
          for: 1h
          labels:
            severity: warning
          annotations:
            summary: "The number of RDS DB clusters is close to its limit."
            description: |
              Remaining DB clusters: {{ with query "acs_fleetshard_central_db_clusters_max - acs_fleetshard_central_db_clusters_used" }}{{ . | first | value | humanize }}{{ end }}. A quota increase should be requested from AWS.
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-029-increase-aws-rds-limits.md"
        - alert: RHACSCentralDBInstancesUtilizationHigh
          expr: |
            acs_fleetshard_central_db_instances_used / acs_fleetshard_central_db_instances_max >= 0.8
          for: 1h
          labels:
            severity: warning
          annotations:
            summary: "The number of RDS DB instances is close to its limit."
            description: |
              Remaining DB instances: {{ with query "acs_fleetshard_central_db_instances_max - acs_fleetshard_central_db_instances_used" }}{{ . | first | value | humanize }}{{ end }}. A quota increase should be requested from AWS.
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-029-increase-aws-rds-limits.md"
        - alert: RHACSCentralDBManualSnapshotsUtilizationHigh
          expr: |
            acs_fleetshard_central_db_snapshots_used / acs_fleetshard_central_db_snapshots_max >= 0.8
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "The number of RDS DB snapshots is close to its limit."
            description: |
              Remaining DB manual snapshots: {{ with query "acs_fleetshard_central_db_snapshots_max - acs_fleetshard_central_db_snapshots_used" }}{{ . | first | value | humanize }}{{ end }}. A quota increase should be requested from AWS.
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-029-increase-aws-rds-limits.md"
        - alert: RHACSCentralDBClustersUtilizationCritical
          expr: |
            acs_fleetshard_central_db_clusters_used / acs_fleetshard_central_db_clusters_max >= 0.9
          for: 1h
          labels:
            severity: critical
          annotations:
            summary: "The number of RDS DB clusters is very close to its limit."
            description: |
              Remaining DB clusters: {{ with query "acs_fleetshard_central_db_clusters_max - acs_fleetshard_central_db_clusters_used" }}{{ . | first | value | humanize }}{{ end }}. A quota increase must be requested from AWS.
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-029-increase-aws-rds-limits.md"
        - alert: RHACSCentralDBInstancesUtilizationCritical
          expr: |
            acs_fleetshard_central_db_instances_used / acs_fleetshard_central_db_instances_max >= 0.9
          for: 1h
          labels:
            severity: critical
          annotations:
            summary: "The number of RDS DB instances is very close to its limit."
            description: |
              Remaining DB instances: {{ with query "acs_fleetshard_central_db_instances_max - acs_fleetshard_central_db_instances_used" }}{{ . | first | value | humanize }}{{ end }}. A quota increase must be requested from AWS.
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-029-increase-aws-rds-limits.md"

    - name: aws-ses
      rules:
        - alert: AWSSESReputationBounceRateTooHigh
          expr: avg_over_time(aws_ses_reputation_bounce_rate_sum[1h]) > 0.05
          labels:
            severity: warning
          annotations:
            summary: 'AWS SES Bounce rate too high'
            description: 'The SES bounce rate is {{ $value | humanizePercentage }}, which is higher than the maximum limit of 5%.'
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-040-aws-ses-violation.md"
        - alert: AWSSESReputationComplaintRateTooHigh
          expr: avg_over_time(aws_ses_reputation_complaint_rate_sum[1h]) > 0.01
          labels:
            severity: warning
          annotations:
            summary: 'AWS SES Complaint rate too high'
            description: 'The SES complaint rate is {{ $value | humanizePercentage }}, which is higher than the maximum limit of 1%.'
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-040-aws-ses-violation.md"
        - alert: AWSSESSendRateTooHigh
          # cloudwatch is setup to collect aws_ses_send_sum in 2 minute intervals (120 seconds)
          # and the max send rate is 14/s.
          expr: (max_over_time(aws_ses_send_sum[1h]) / 120) > 12
          labels:
              severity: warning
          annotations:
              summary: 'AWS SES Send rate too high'
              description: 'The maximum send rate over the last hour is {{ $value }} messages/second, which is dangerously approaching the maximum limit of 14 per second.'
              sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-040-aws-ses-violation.md"

    - name: deadmanssnitch
      rules:
        - alert: DeadMansSwitch
          annotations:
            message: >
              This is an alert meant to ensure that the entire alerting pipeline is functional.
              This alert is always firing, therefore it should always be firing in Alertmanager
              and always fire against a receiver. There are integrations with various notification
              mechanisms that send a notification when this alert is not firing. For example the
              "DeadMansSnitch" integration in PagerDuty.
          expr: vector(1)
          labels:
            name: DeadMansSwitchAlert

    - name: federation
      rules:
        - alert: OpenshiftMonitoringFederationScrapeTargetDown
          expr: up{job="openshift-monitoring-federation"} != 1 or absent(up{job="openshift-monitoring-federation"})
          for: 10m
          labels:
            severity: critical
          annotations:
            summary: "Openshift monitoring federation scrape target is down."
            description: "The Openshift monitoring federation scrape target has been down for longer than 10 minutes."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-015-openshift-monitoring-federation-unavailable.md"

    - name: observability-operator
      rules:
        - alert: ObservabilityOperatorPrometheusPersistentVolumeFillingUp
          expr: kubelet_volume_stats_available_bytes{persistentvolumeclaim=~"managed-services-prometheus-obs-prometheus-[0-9]"} / kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~"managed-services-prometheus-obs-prometheus-[0-9]"} < 0.1
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "The Observability Operator's Prometheus persistent volume in namespace namespace `{{ $labels.namespace }}` is filling up."
            description: "The Observability Operator's Prometheus storage in namespace `{{ $labels.namespace }}` is filling up for PVC `{{ $labels.persistentvolumeclaim }}`. Available storage quota is `{{ $value | humanizePercentage }}`."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-012-increasing-observability-operator-pvc-size.md"

        - alert: ObservabilityOperatorPrometheusPersistentVolumeFillingUp
          expr: kubelet_volume_stats_available_bytes{persistentvolumeclaim=~"managed-services-prometheus-obs-prometheus-[0-9]"} / kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~"managed-services-prometheus-obs-prometheus-[0-9]"} < 0.25 and predict_linear(kubelet_volume_stats_available_bytes{persistentvolumeclaim=~"managed-services-prometheus-obs-prometheus-[0-9]"}[6h], 4 * 24 * 3600) < 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "The Observability Operator's Prometheus persistent volume in namespace namespace `{{ $labels.namespace }}` is filling up."
            description: "The Observability Operator's Prometheus storage in namespace `{{ $labels.namespace }}` is filling up for PVC `{{ $labels.persistentvolumeclaim }}`. Available storage quota is `{{ $value | humanizePercentage }}`. The volume is expected to fill up within 4 days based on linear extrapolation over the last 6 hours."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-012-increasing-observability-operator-pvc-size.md"

        - alert: ObservabilityOperatorRemoteWriteFailure
          expr: >-
            obs_operator:prometheus_remote_storage_succeeded_samples:ratio_rate1h < 0.25
          for: 1h
          labels:
            severity: critical
          annotations:
            summary: "The Observability Operator's Prometheus is failing to remote write to Observatorium."
            description: "The Prometheus remote write success rate is `{{ $value | humanizePercentage }}`."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-013-observability-operator-write-failure.md"

    - name: aws-rds-acuutilization
      rules:
        - alert: AWSRDSACUUtilization
          expr: |
            aws_rds_acuutilization_average{dimension_DBInstanceIdentifier=~"rhacs-.*"} > 95
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "The AWS RDS ACUUtilization for `{{ $labels.dimension_DBInstanceIdentifier }}` DB instance is too high."
            description: >
              The DB instance `{{ $labels.dimension_DBInstanceIdentifier }}` has scaled up as high as it can.
              Consider increasing the maximum ACU setting for the cluster.
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-014-aws-rds-acu-utilization.md"

    - name: rhacs-central.sli
      rules:
        # Pod ready SLI
        # Central pod must be ready to count as available.
        # This is a time series of 0s (down) and 1s (up).
        - expr: |
            label_replace(
              clamp_max(
                sum by(namespace) (kube_deployment_status_replicas_ready{deployment="central", namespace=~"rhacs-.*"}), 1
              ),
              "rhacs_instance_id",
              "$1",
              "namespace",
              "rhacs-(.*)"
            )
          record: central:sli:pod_ready

        # Error rate SLI
        # The error rate over the last 10 minutes must be smaller than 35% to count as available.

        # GRPC
        # TODO(ROX-19917): Re-add `grpc_code="Unknown"` to the list of server errors.
        - expr: |
            sum by (namespace, rhacs_instance_id, rhacs_org_id, rhacs_org_name, rhacs_cluster_name, rhacs_environment)
            (rate(grpc_server_handled_total{namespace=~"rhacs-.*", job="central", grpc_type="unary", grpc_service!="v1.PingService", grpc_code!~"DeadlineExceeded|Internal|Unavailable"}[10m]))
          record: central:grpc_server_handled:server_available_code:rate10m

        - expr: |
            sum by (namespace, rhacs_instance_id, rhacs_org_id, rhacs_org_name, rhacs_cluster_name, rhacs_environment)
            (rate(grpc_server_handled_total{namespace=~"rhacs-.*", job="central", grpc_type="unary",grpc_service!="v1.PingService"}[10m]))
          record: central:grpc_server_handled:total:rate10m

        # HTTP

        # need to filter exported_namespace by `rhacs-[a-zA-Z0-9]{20}` to remove rhacs-observability namespace.

        # exported_namespace exists because of a label collision with the namespace the metrics are collected from (which is the namespace in which the router is deployed)
        # the namespace we care about needs to be relabeled to be joined up with the grpc timeseries
        - expr: |
            label_replace(
              sum by (exported_namespace)
              (rate(haproxy_backend_http_responses_total{exported_namespace=~"rhacs-[a-zA-Z0-9]{20}",code!~"5xx"}[10m])),
              "namespace", "$1", "exported_namespace", "(.+)"
            )
          record: central:http_incoming_requests:not_5xx:rate10m

        - expr: |
            label_replace(
              sum by (exported_namespace)
              (rate(haproxy_backend_http_responses_total{exported_namespace=~"rhacs-[a-zA-Z0-9]{20}"}[10m])),
              "namespace", "$1", "exported_namespace", "(.+)"
            )
          record: central:http_incoming_requests:total:rate10m

        - expr: |
            central:http_incoming_requests:not_5xx:rate10m
              + on (namespace) group_left(rhacs_instance_id) central:grpc_server_handled:server_available_code:rate10m
          record: central:incoming_requests:available:rate10m

        - expr: |
            central:http_incoming_requests:total:rate10m
              + on (namespace) group_left(rhacs_instance_id) central:grpc_server_handled:total:rate10m
          record: central:incoming_requests:total:rate10m

        - expr: |
            clamp (
              central:incoming_requests:available:rate10m
                /
              (central:incoming_requests:total:rate10m > 0),
              0, 1
            )
          record: central:success_rate10m

        # This is a time series of 0s (down) and 1s (up).
        # Success rate above 65% is floored to 1.
        # Success rate below 65% is floored to 0.

        # If no requests have been registered in the rate period, we default to `central:sli:pod_ready`.
        # The fallback mechanism helps to avoid undefined values in the metric series, which are otherwise simply
        # dropped by Prometheus, and would therefore mess up the aggregated `central:sli:availability` metric series.
        # We keep the `namespace` and `rhacs_instance_id` labels to be consistent with other SLI metrics and for
        # reference in dashboards.
        - expr: |
            max by (namespace, rhacs_instance_id) (
                central:success_rate10m > bool 0.65
            )
            or on (namespace, rhacs_instance_id) central:sli:pod_ready
          record: central:sli:error_rate

        # Availability SLI
        # If any of the SLIs is violated, the service counts as unavailable.
        # This is a time series of 0s (down) and 1s (up).
        - expr: |
            central:sli:pod_ready * on (namespace, rhacs_instance_id) central:sli:error_rate
          record: central:sli:availability

        - expr: |
            sum by (namespace, rhacs_instance_id) (count_over_time(central:sli:availability[1h]))
          record: central:sli:availability:count_over_time1h

        - expr: |
            sum by (namespace, rhacs_instance_id) (count_over_time(central:sli:availability[28d]))
          record: central:sli:availability:count_over_time28d

        - expr: |
            sum by (namespace, rhacs_instance_id) (sum_over_time(central:sli:availability[1h]))
          record: central:sli:availability:sum_over_time1h

        - expr: |
            sum by (namespace, rhacs_instance_id) (sum_over_time(central:sli:availability[28d]))
          record: central:sli:availability:sum_over_time28d

        # Extended average over time refers to the time series effectively being extended
        # over the entire time interval. This is in contrast to `avg_over_time`, which
        # only averages over time intervals where the time series is not nil.
        # This is important during the initial 28 days of Central instances. For example,
        # consider a Central instance that lived for 5 minutes and was down for 2 minutes.
        # Using `avg_over_time`, the availability would be `3 min / 5 min = 60%`. The
        # extended average over 28 days would yield `1 - 2 min / 28 days ~ 99.995%`.
        # After the initial 28 days, both averages are equivalent.
        - expr: |
            clamp(
              1 - (central:sli:availability:count_over_time1h - central:sli:availability:sum_over_time1h) / scalar(central:slo:scrapes1h),
              0,
              1
            )
          record: central:sli:availability:extended_avg_over_time1h

        - expr: |
            clamp(
              1 - (central:sli:availability:count_over_time28d - central:sli:availability:sum_over_time28d) / scalar(central:slo:scrapes28d),
              0,
              1
            )
          record: central:sli:availability:extended_avg_over_time28d

    - name: rhacs-central.slo
      rules:
        # Availability SLO
        - expr: "0.99"
          record: central:slo:availability

        # Based on 30s scrape intervals.
        - expr: "60 * 2"
          record: central:slo:scrapes1h

        - expr: "28 * 24 * 60 * 2"
          record: central:slo:scrapes28d

        # 0% exhaustion means no recorded failures.
        # 100% exhaustion means the SLO target has been reached.
        # >100% exhaustion means the SLO target has been violated.
        - expr: |
            (1 - central:sli:availability:extended_avg_over_time28d) / (1 - scalar(central:slo:availability))
          record: central:slo:availability:error_budget_exhaustion

        # A burn rate of 1 corresponds to full error budget exhaustion after the SLO window W.
        # A burn rate of n corresponds to full error budget exhaustion after W/n - in other words,
        # it measures the exhaustion velocity. To keep the SLO target, a temporary burn rate larger
        # than 1 must be compensated with a burn rate smaller than 1.
        - expr: |
            (1 - central:sli:availability:extended_avg_over_time1h) / (1 - scalar(central:slo:availability))
          record: central:slo:availability:burnrate1h

    - name: rhacs-central.alerts
      rules:
        - alert: Central availability error budget exhaustion - 90%
          annotations:
            message: "High availability error budget exhaustion for central. Current exhaustion: {{ $value | humanizePercentage }}."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-018-rhacs-central-slo-alerts.md"
          expr: |
            central:slo:availability:error_budget_exhaustion >= 0.9 and central:sli:availability >= 0
          labels:
            service: central
            severity: critical
            namespace: "{{ $labels.namespace }}"
            rhacs_instance_id: "{{ $labels.rhacs_instance_id }}"

        - alert: Central availability error budget exhaustion - 70%
          annotations:
            message: "High availability error budget exhaustion for central. Current exhaustion: {{ $value | humanizePercentage }}."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-018-rhacs-central-slo-alerts.md"
          expr: |
            central:slo:availability:error_budget_exhaustion >= 0.7 and central:sli:availability >= 0
          labels:
            service: central
            severity: warning
            namespace: "{{ $labels.namespace }}"
            rhacs_instance_id: "{{ $labels.rhacs_instance_id }}"

        - alert: Central availability error budget exhaustion - 50%
          annotations:
            message: "High availability error budget exhaustion for central. Current exhaustion: {{ $value | humanizePercentage }}."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-018-rhacs-central-slo-alerts.md"
          expr: |
            central:slo:availability:error_budget_exhaustion >= 0.5 and central:sli:availability >= 0
          labels:
            service: central
            severity: warning
            namespace: "{{ $labels.namespace }}"
            rhacs_instance_id: "{{ $labels.rhacs_instance_id }}"

        - alert: Central availability weekly exhaustion
          annotations:
            message: "Availability error budget exhaustion has increased by {{ $value | humanizePercentage }} over the last week."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-018-rhacs-central-slo-alerts.md"
          expr: |
            central:slo:availability:error_budget_exhaustion - central:slo:availability:error_budget_exhaustion offset 1w > 0.1
          labels:
            service: central
            severity: warning
            namespace: "{{ $labels.namespace }}"
            rhacs_instance_id: "{{ $labels.rhacs_instance_id }}"

        - alert: Central high availability burn rate
          annotations:
            message: "High availability burn rate for central. Current burn rate per hour: {{ $value | humanize }}."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-018-rhacs-central-slo-alerts.md"
          # Corresponds to less than 50% up time over 1 hour assuming a 99% SLO target.
          # See recording rules for how burn rates relate to SLI and SLO in general.
          expr: |
            central:slo:availability:burnrate1h >= 50 and central:sli:availability >= 0
          labels:
            service: central
            severity: critical
            namespace: "{{ $labels.namespace }}"
            rhacs_instance_id: "{{ $labels.rhacs_instance_id }}"
    - name: az-resources
      rules:
        - record: acscs_worker_nodes
          expr: |
            kube_node_role{role="acscs-worker"}
        - record: node_availability_zone
          expr: |
            sum(label_replace(kube_node_labels, "availability_zone", "$1", "label_failure_domain_beta_kubernetes_io_zone", "(.*)")) by (availability_zone, node) > 0
        - record: memory_resource_requests:acscs_worker_nodes:by_availability_zone:sum
          expr: |
            sum(
              sum(cluster:namespace:pod_memory:active:kube_pod_container_resource_requests{resource="memory",observability="",job="kube-state-metrics"}) by (node)
              * on (node) acscs_worker_nodes
              * on (node) group_left(availability_zone) node_availability_zone
            ) by (availability_zone)
        - record: memory_resource_limits:acscs_worker_nodes:by_availability_zone:sum
          expr: |
            sum(
              sum(cluster:namespace:pod_memory:active:kube_pod_container_resource_limits{resource="memory",observability="",job="kube-state-metrics"}) by (node)
              * on (node) acscs_worker_nodes
              * on (node) group_left(availability_zone) node_availability_zone
            ) by (availability_zone)
        - record: cpu_resource_requests:acscs_worker_nodes:by_availability_zone:sum
          expr: |
            sum(
              sum(cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests{resource="cpu", observability="",job="kube-state-metrics"}) by (node)
              * on (node) acscs_worker_nodes
              * on (node) group_left(availability_zone) node_availability_zone
            ) by (availability_zone)
        - record: cpu_resource_limits:acscs_worker_nodes:by_availability_zone:sum
          expr: |
            sum(
              sum(cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits{observability="",job="kube-state-metrics"}) by (node)
              * on (node) acscs_worker_nodes
              * on (node) group_left(availability_zone) node_availability_zone
            ) by (availability_zone)
        - record: availability_zone:acscs_worker_nodes:allocatable_cpu
          expr: |
            sum(
              sum(kube_node_status_allocatable{resource="cpu"}) by (node)
              * on (node) acscs_worker_nodes
              * on (node) group_left(availability_zone) node_availability_zone
            ) by (availability_zone)
        - record: availability_zone:acscs_worker_nodes:allocatable_memory
          expr: |
            sum(
              sum(kube_node_status_allocatable{resource="memory"}) by (node)
              * on (node) acscs_worker_nodes
              * on (node) group_left(availability_zone) node_availability_zone
            ) by (availability_zone)
        - record: availability_zone:acscs_worker_nodes:memory_request_ratio
          expr: |
            memory_resource_requests:acscs_worker_nodes:by_availability_zone:sum
            /
            availability_zone:acscs_worker_nodes:allocatable_memory
        - record: availability_zone:acscs_worker_nodes:cpu_request_ratio
          expr: |
            cpu_resource_requests:acscs_worker_nodes:by_availability_zone:sum
            /
            availability_zone:acscs_worker_nodes:allocatable_cpu
        - record: availability_zone:acscs_worker_nodes:memory_limit_ratio
          expr: |
            memory_resource_limits:acscs_worker_nodes:by_availability_zone:sum
            /
            availability_zone:acscs_worker_nodes:allocatable_memory
        - record: availability_zone:acscs_worker_nodes:cpu_limit_ratio
          expr: |
            cpu_resource_limits:acscs_worker_nodes:by_availability_zone:sum
            /
            availability_zone:acscs_worker_nodes:allocatable_cpu
        - alert: WorkerNodesMemoryQuotaOverCommit
          expr: avg(availability_zone:acscs_worker_nodes:memory_request_ratio) > 0.99
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "There is a risk of over-committing Memory resources on worker nodes."
            description: "During the last 15 minutes, the average memory request commitment on worker nodes was {{ $value | humanizePercentage }}. This could make pods unschedulable."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-cloud-service/runbooks/-/blob/master/sops/dp-027-cluster-scale-up.md"
        - alert: WorkerNodesCPUQuotaOverCommit
          expr: avg(availability_zone:acscs_worker_nodes:cpu_request_ratio) > 0.99
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "There is a risk of over-committing CPU resources on worker nodes."
            description: "During the last 15 minutes, the average CPU request commitment on worker nodes was {{ $value | humanizePercentage }}. This could make pods unschedulable."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-cloud-service/runbooks/-/blob/master/sops/dp-027-cluster-scale-up.md"
        - alert: WorkerNodesMemoryOverCommit
          expr: avg(availability_zone:acscs_worker_nodes:memory_limit_ratio) > 2
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "There is a high risk of over-committing Memory resources on worker nodes."
            description: "During the last 5 minutes, the average Memory limit commitment on worker nodes was {{ $value | humanizePercentage }}. This is above the recommended threshold of 200%."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-cloud-service/runbooks/-/blob/master/sops/dp-027-cluster-scale-up.md"
    - name: cluster-autoscaler
      rules:
        # Copied from: https://github.com/openshift/cluster-autoscaler-operator/blob/287595b0ee37b893c02d51d3a461ba118b90c3dc/pkg/controller/clusterautoscaler/monitoring.go#L153
        # However, different severity levels are used to ensure timely response.
        # Additional information about alerts: https://github.com/openshift/cluster-autoscaler-operator/blob/287595b0ee37b893c02d51d3a461ba118b90c3dc/docs/user/alerts.md
        - alert: ClusterAutoscalerUnschedulablePods
          expr: cluster_autoscaler_unschedulable_pods_count{service="cluster-autoscaler-default"} > 0
          for: 30m
          labels:
            severity: critical
          annotations:
            summary: "Cluster Autoscaler has {{ $value }} unschedulable pods."
            description: "The cluster autoscaler is unable to scale up and is alerting that there are unschedulable pods because of this condition.
This may be caused by the cluster autoscaler reaching its resources limits, or by Kubernetes waiting for new nodes to become ready."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-cloud-service/runbooks/-/blob/master/sops/dp-042-modify-cluster-autoscaler.md"
        - alert: ClusterAutoscalerNotSafeToScale
          expr: cluster_autoscaler_cluster_safe_to_autoscale{service="cluster-autoscaler-default"} != 1
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "Cluster Autoscaler is reporting that the cluster is not ready for scaling."
            description: "The cluster autoscaler has detected that the number of unready nodes is too high
and it is not safe to continute scaling operations. It makes this determination by checking that the number of ready nodes is greater than the minimum ready count
(default of 3) and the ratio of unready to ready nodes is less than the maximum unready node percentage (default of 45%). If either of those conditions are not
true then the cluster autoscaler will enter an unsafe to scale state until the conditions change."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-cloud-service/runbooks/-/blob/master/sops/dp-042-modify-cluster-autoscaler.md"
        - alert: ClusterAutoscalerUnableToScaleCPULimitReached
          expr: increase(cluster_autoscaler_skipped_scale_events_count{service="cluster-autoscaler-default",direction="up",reason="CpuResourceLimit"}[15m]) > 0
          for: 15m
          labels:
            severity: info
          annotations:
            summary: "Cluster Autoscaler has reached its maximum CPU core limit and is unable to scale out."
            description: "The number of total cores in the cluster has exceeded the maximum number set on the
cluster autoscaler. This is calculated by summing the cpu capacity for all nodes in the cluster and comparing that number against the maximum cores value set for the
cluster autoscaler. Limits can be adjusted by modifying the cluster autoscaler configuration."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-cloud-service/runbooks/-/blob/master/sops/dp-042-modify-cluster-autoscaler.md"
        - alert: ClusterAutoscalerUnableToScaleMemoryLimitReached
          expr: increase(cluster_autoscaler_skipped_scale_events_count{service="cluster-autoscaler-default",direction="up",reason="MemoryResourceLimit"}[15m]) > 0
          for: 15m
          labels:
            severity: info
          annotations:
            summary: "Cluster Autoscaler has reached its maximum Memory bytes limit and is unable to scale out."
            description: "The number of total bytes of RAM in the cluster has exceeded the maximum number set on
the cluster autoscaler. This is calculated by summing the memory capacity for all nodes in the cluster and comparing that number against the maximum memory bytes value set
for the cluster autoscaler. Limits can be adjusted by modifying the cluster autoscaler configuration."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-cloud-service/runbooks/-/blob/master/sops/dp-042-modify-cluster-autoscaler.md"
        - alert: ClusterAuditSELinuxViolations
          expr: |
            selinux_denials_sample_count > 0
          labels:
            severity: info
          annotations:
            summary: "SELinux Violations occuring on cluster."
            description: |
              A cluster node logged {{ $value }} SELinux AVC denial(s) per minute to the audit log.
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-cloud-service/runbooks/-/blob/master/sops/dp-043-selinux-violation.md"
        - alert: ClusterAuditNetworkPolicyViolations
          expr: |
            network_policy_denials_sample_count > 0
          for: 10m
          labels:
            severity: info
          annotations:
            summary: "Network Policy Violations occuring on cluster."
            description: |
              A cluster node logged Network Policy ACL denial(s) for 10 minutes.
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-cloud-service/runbooks/-/blob/master/sops/dp-044-network-policy-violation.md"
        - alert: ClusterAuditNetworkPolicyViolations
          expr: |
            network_policy_denials_sample_count >= 15
          for: 1m
          labels:
            severity: info
          annotations:
            summary: "Network Policy Violations occuring on cluster."
            description: |
              A cluster node logged at least {{ $value }} Network Policy ACL denial(s) per minute.
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-cloud-service/runbooks/-/blob/master/sops/dp-044-network-policy-violation.md"
