apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    app: rhacs
  name: rhacs-data-plane-prometheus-rules
spec:
  groups:
    - name: rhacs-central
      rules:
        - alert: RHACSCentralScrapeFailed
          expr: |
            avg_over_time(up{pod=~"central-.*"}[10m]) < 0.5 and ON(pod) kube_pod_container_status_ready{container="central"} == 1
          for: 20m
          labels:
            severity: critical
          annotations:
            summary: "Prometheus unable to scrape metrics from target `{{ $labels.pod }}` in namespace `{{ $labels.namespace }}`."
            description: "During the last 10 minutes, only `{{ $value | humanizePercentage }}` of scrapes of target `{{ $labels.pod }}` in namespace `{{ $labels.namespace }}` were successful. This alert is raised when less than 50% of scrapes are successful."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-003-rhacs-instance-unavailable.md"
        - alert: RHACSCentralContainerDown
          expr: |
            avg_over_time(kube_pod_container_status_ready{container="central"}[10m]) < 0.5
          for: 20m
          labels:
            severity: critical
          annotations:
            summary: "Central container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` is down or in a CrashLoopBackOff status."
            description: "Central container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` has been down or in a CrashLoopBackOff status for at least 10 minutes."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-003-rhacs-instance-unavailable.md"
        - alert: RHACSCentralContainerFrequentlyRestarting
          expr: |
            increase(kube_pod_container_status_restarts_total{container="central"}[30m]) > 3
          labels:
            severity: critical
          annotations:
            summary: "Central container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` restarted more than 3 times."
            description: "Central container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` has restarted more than 3 times during the last 30 minutes."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-003-rhacs-instance-unavailable.md"
        - alert: RHACSCentralDatabasePersistentVolumeFillingUp
          expr: kubelet_volume_stats_available_bytes{persistentvolumeclaim="stackrox-db"} / kubelet_volume_stats_capacity_bytes{persistentvolumeclaim="stackrox-db"} < 0.1
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Central database storage in namespace `{{ $labels.namespace }}` is filing up."
            description: "Central database storage in namespace `{{ $labels.namespace }}` is filling up for PVC `{{ $labels.persistentvolumeclaim }}`. Available storage quota is `{{ $value | humanizePercentage }}`."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-006-increasing-rhacs-pvc-size.md"
        - alert: RHACSCentralDatabasePersistentVolumeFillingUp
          expr: kubelet_volume_stats_available_bytes{persistentvolumeclaim="stackrox-db"} / kubelet_volume_stats_capacity_bytes{persistentvolumeclaim="stackrox-db"} < 0.25 and predict_linear(kubelet_volume_stats_available_bytes{persistentvolumeclaim="stackrox-db"}[6h], 4 * 24 * 3600) < 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Central database storage in namespace `{{ $labels.namespace }}` is filing up."
            description: "Central database storage in namespace `{{ $labels.namespace }}` is filling up for PVC `{{ $labels.persistentvolumeclaim }}`. Available storage quota is `{{ $value | humanizePercentage }}`. The volume is expected to fill up within 4 days based on linear extrapolation over the last 6 hours."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-006-increasing-rhacs-pvc-size.md"
        - alert: RHACSCentralPostgresConnectionDown
          expr: avg_over_time(rox_central_postgres_connected{container="central"}[10m]) < 0.5
          for: 20m
          labels:
            severity: critical
          annotations:
            summary: "Central container `{{ $labels.pod }}/{{ $labels.container }}` database connection in namespace `{{ $labels.namespace }}` is down or is in a bad shape."
            description: "Central container `{{ $labels.pod }}/{{ $labels.container }}` database connection in namespace `{{ $labels.namespace }}` has been down or is in a bad shape for at least 10 minutes."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-010-rhacs-instance-db-unavailable.md"

    - name: rhacs-scanner
      rules:
        - alert: RHACSScannerScrapeFailed
          expr: |
            avg_over_time(up{pod=~"scanner-.*"}[10m]) < 0.5 and ON(pod) kube_pod_container_status_ready{pod=~"scanner.*", container=~"scanner|db"} == 1
          for: 20m
          labels:
            severity: critical
          annotations:
            summary: "Prometheus unable to scrape metrics from target `{{ $labels.pod }}` in namespace `{{ $labels.namespace }}`."
            description: "During the last 10 minutes, only `{{ $value | humanizePercentage }}` of scrapes of target `{{ $labels.pod }}` in namespace `{{ $labels.namespace }}` were successful. This alert is raised when less than 50% of scrapes are successful."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-003-rhacs-instance-unavailable.md"
        - alert: RHACSScannerContainerDown
          expr: |
            avg_over_time(kube_pod_container_status_ready{pod=~"scanner.*", container=~"scanner|db"}[10m]) < 0.5
          for: 20m
          labels:
            severity: critical
          annotations:
            summary: "Scanner container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` is down or in a CrashLoopBackOff status."
            description: "Scanner container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` has been down or in a CrashLoopBackOff status for at least 10 minutes."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-003-rhacs-instance-unavailable.md"
        - alert: RHACSScannerContainerFrequentlyRestarting
          expr: |
            increase(kube_pod_container_status_restarts_total{pod=~"scanner.*", container=~"scanner|db"}[30m]) > 3
          labels:
            severity: critical
          annotations:
            summary: "Scanner container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` restarted more than 3 times."
            description: "Scanner container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` has restarted more than 3 times during the last 30 minutes."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-003-rhacs-instance-unavailable.md"

    - name: rhacs-fleetshard
      rules:
        - alert: RHACSFleetshardOperatorContainerDown
          expr: |
            avg_over_time(kube_pod_container_status_ready{pod=~"rhacs-operator-controller-manager-.*"}[10m]) < 0.5
          for: 20m
          labels:
            severity: critical
          annotations:
            summary: "Fleetshard operator container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` is down or in a CrashLoopBackOff status."
            description: "Fleetshard operator container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` has been down or in a CrashLoopBackOff status for at least 10 minutes."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-011-rhacs-operator-unavailable.md"
        - alert: RHACSFleetshardOperatorContainerFrequentlyRestarting
          expr: |
            increase(kube_pod_container_status_restarts_total{pod=~"rhacs-operator-controller-manager-.*"}[30m]) > 3
          labels:
            severity: critical
          annotations:
            summary: "Fleetshard operator container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` restarted more than 3 times."
            description: "Fleetshard operator container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` has restarted more than 3 times during the last 30 minutes."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-011-rhacs-operator-unavailable.md"
        - alert: RHACSFleetshardSyncScrapeFailed
          expr: |
            (avg_over_time(up{pod=~"fleetshard-sync-.*"}[10m]) < 0.5 and ON(pod) kube_pod_container_status_ready{pod=~"fleetshard-sync-.*"} == 1) or absent(up{pod=~"fleetshard-sync-.*"})
          for: 20m
          labels:
            severity: critical
          annotations:
            summary: "Prometheus unable to scrape metrics from target `{{ $labels.pod }}` in namespace `{{ $labels.namespace }}`."
            description: "During the last 10 minutes, only `{{ $value | humanizePercentage }}` of scrapes of target `{{ $labels.pod }}` in namespace `{{ $labels.namespace }}` were successful. This alert is raised when less than 50% of scrapes are successful."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-005-fleetshard-sync-unavailable.md"
        - alert: RHACSFleetshardSyncContainerDown
          expr: |
            avg_over_time(kube_pod_container_status_ready{pod=~"fleetshard-sync-.*"}[10m]) < 0.5
          for: 20m
          labels:
            severity: critical
          annotations:
            summary: "Fleetshard synchronizer container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` is down or in a CrashLoopBackOff status."
            description: "Fleetshard synchronizer container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` has been down or in a CrashLoopBackOff status for at least 10 minutes."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-005-fleetshard-sync-unavailable.md"
        - alert: RHACSFleetshardSyncContainerFrequentlyRestarting
          expr: increase(kube_pod_container_status_restarts_total{pod=~"fleetshard-sync-.*"}[30m]) > 3
          labels:
            severity: critical
          annotations:
            summary: "Fleetshard synchronizer container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` restarted more than 3 times."
            description: "Fleetshard synchronizer container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` has restarted more than 3 times during the last 30 minutes."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-005-fleetshard-sync-unavailable.md"
        - alert: RHACSFleetshardSyncReconciliationErrors
          expr: |
            acs_fleetshard_central_errors_per_reconciliations:ratio_rate10m > 0.01
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Fleetshard synchronizer container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` failed to reconcile Central instances."
            description: "Fleetshard synchronizer container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` has a reconciliation error rate of {{ $value | humanizePercentage }} over the last 10 minutes."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-007-fleetshard-sync-reconciliation-error.md"
        - alert: RHACSFleetshardSyncFleetManagerRequestErrors
          expr: |
            acs_fleetshard_fleet_manager_errors_per_requests:ratio_rate10m > 0.01
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Fleetshard synchronizer container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` failed to reach fleet manager."
            description: "Fleetshard synchronizer container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` has a fleet manager request error rate of {{ $value | humanizePercentage }} over the last 10 minutes."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-007-fleetshard-sync-reconciliation-error.md"
        - alert: RHACSFleetshardSyncInvalidCentralCount
          expr: |
            sum(acs_fleetshard_total_centrals) <= 0
          for: 15m
          labels:
            severity: critical
          annotations:
            summary: "Fleetshard synchronizer manages `{{ $value }}` centrals."
            description: "Fleetshard synchronizer manages `{{ $value }}` centrals. The number of Centrals should always be larger than zero in a working system. If it drops to or below zero, fleetshard synchronizer is assumed to be in a failed state."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-007-fleetshard-sync-reconciliation-error.md"

    - name: rhacs-probe
      rules:
        - alert: RHACSProbeRunFailed
          expr: acs_probe_last_failure_timestamp > 0 and acs_probe_last_failure_timestamp >= acs_probe_last_success_timestamp
          for: 30m
          labels:
            severity: critical
          annotations:
            summary: "The latest probe run failed at `{{ $value | humanizeTimestamp }}`."
            description: "The latest run of probe `{{ $labels.pod }}` failed at `{{ $value | humanizeTimestamp }}`."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-008-probe-run-failed.md"
        - alert: RHACSProbeScrapeFailed
          expr: |
            avg_over_time(up{job="probe"}[10m]) < 0.5 and ON(pod) kube_pod_container_status_ready{container="probe"} == 1
          for: 20m
          labels:
            severity: critical
          annotations:
            summary: "Prometheus unable to scrape metrics from target `{{ $labels.pod }}` in namespace `{{ $labels.namespace }}`."
            description: "During the last 10 minutes, only `{{ $value | humanizePercentage }}` of scrapes of target `{{ $labels.pod }}` in namespace `{{ $labels.namespace }}` were successful. This alert is raised when less than 50% of scrapes are successful."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-009-probe-unavailable.md"
        - alert: RHACSProbeContainerDown
          expr: |
            avg_over_time(kube_pod_container_status_ready{container="probe"}[10m]) < 0.5
          for: 20m
          labels:
            severity: critical
          annotations:
            summary: "Probe container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` is down or in a CrashLoopBackOff status."
            description: "Probe container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` has been down or in a CrashLoopBackOff status for at least 10 minutes."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-009-probe-unavailable.md"
        - alert: RHACSProbeContainerFrequentlyRestarting
          expr: |
            increase(kube_pod_container_status_restarts_total{container="probe"}[30m]) > 3
          labels:
            severity: critical
          annotations:
            summary: "Probe container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` restarted more than 3 times."
            description: "Probe container `{{ $labels.pod }}/{{ $labels.container }}` in namespace `{{ $labels.namespace }}` has restarted more than 3 times during the last 30 minutes."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-009-probe-unavailable.md"

    - name: deadmanssnitch
      rules:
        - alert: DeadMansSwitch
          annotations:
            message: >
              This is an alert meant to ensure that the entire alerting pipeline is functional.
              This alert is always firing, therefore it should always be firing in Alertmanager
              and always fire against a receiver. There are integrations with various notification
              mechanisms that send a notification when this alert is not firing. For example the
              "DeadMansSnitch" integration in PagerDuty.
          expr: vector(1)
          labels:
            name: DeadMansSwitchAlert

    - name: federation
      rules:
        - alert: OpenshiftMonitoringFederationScrapeTargetDown
          expr: up{job="openshift-monitoring-federation"} != 1 or absent(up{job="openshift-monitoring-federation"})
          for: 10m
          labels:
            severity: critical
          annotations:
            summary: "Openshift monitoring federation scrape target is down."
            description: "The Openshift monitoring federation scrape target has been down for longer than 10 minutes."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-015-openshift-monitoring-federation-unavailable.md"

    - name: observability-operator
      rules:
        - alert: ObservabilityOperatorPrometheusPersistentVolumeFillingUp
          expr: kubelet_volume_stats_available_bytes{persistentvolumeclaim=~"managed-services-prometheus-obs-prometheus-[0-9]"} / kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~"managed-services-prometheus-obs-prometheus-[0-9]"} < 0.1
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "The Observability Operator's Prometheus persistent volume in namespace namespace `{{ $labels.namespace }}` is filling up."
            description: "The Observability Operator's Prometheus storage in namespace `{{ $labels.namespace }}` is filling up for PVC `{{ $labels.persistentvolumeclaim }}`. Available storage quota is `{{ $value | humanizePercentage }}`."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-012-increasing-observability-operator-pvc-size.md"

        - alert: ObservabilityOperatorPrometheusPersistentVolumeFillingUp
          expr: kubelet_volume_stats_available_bytes{persistentvolumeclaim=~"managed-services-prometheus-obs-prometheus-[0-9]"} / kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~"managed-services-prometheus-obs-prometheus-[0-9]"} < 0.25 and predict_linear(kubelet_volume_stats_available_bytes{persistentvolumeclaim=~"managed-services-prometheus-obs-prometheus-[0-9]"}[6h], 4 * 24 * 3600) < 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "The Observability Operator's Prometheus persistent volume in namespace namespace `{{ $labels.namespace }}` is filling up."
            description: "The Observability Operator's Prometheus storage in namespace `{{ $labels.namespace }}` is filling up for PVC `{{ $labels.persistentvolumeclaim }}`. Available storage quota is `{{ $value | humanizePercentage }}`. The volume is expected to fill up within 4 days based on linear extrapolation over the last 6 hours."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-012-increasing-observability-operator-pvc-size.md"

        - alert: ObservabilityOperatorRemoteWriteFailure
          expr: >-
            obs_operator:prometheus_remote_storage_succeeded_samples:ratio_rate1h < 0.25
          for: 1h
          labels:
            severity: critical
          annotations:
            summary: "The Observability Operator's Prometheus is failing to remote write to Observatorium."
            description: "The Prometheus remote write success rate is `{{ $value | humanizePercentage }}`."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-013-observability-operator-write-failure.md"

    - name: aws-rds-acuutilization
      rules:
        - alert: AWSRDSACUUtilization
          expr: |
            avg_over_time(aws_rds_acuutilization_maximum[5m]) > 95
          for: 1h
          labels:
            severity: critical
          annotations:
            summary: "The AWS RDS ACUUtilization for `{{ $labels.dimension_DBInstanceIdentifier }}` DB instance is too high."
            description: >
              The DB instance `{{ $labels.dimension_DBInstanceIdentifier }}` has scaled up as high as it can.
              Consider increasing the maximum ACU setting for the cluster.
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-014-aws-rds-acu-utilization.md"

    - name: rhacs-central.sli
      rules:
        # Pod ready SLI
        # Central pod must be ready to count as available.
        # This is a time series of 0s (down) and 1s (up).
        - expr: |
            clamp_max(sum by(namespace) (kube_pod_container_status_ready{container="central", pod=~"central-.*", namespace=~"rhacs-.*"}), 1) or vector(0)
          record: central:sli:pod_ready

        # Error rate SLI
        # The error rate over the last 10 minutes must be smaller than 35% to count as available.

        # GRPC
        - expr: |
            sum by (namespace, rhacs_instance_id, rhacs_org_id, rhacs_org_name, rhacs_cluster_name, rhacs_environment)
            (rate(grpc_server_handled_total{namespace=~"rhacs-.*", job="central", grpc_type="unary", grpc_code="OK"}[10m]))
          record: central:grpc_server_handled:OK:rate10m

        - expr: |
            sum by (namespace, rhacs_instance_id, rhacs_org_id, rhacs_org_name, rhacs_cluster_name, rhacs_environment)
            (rate(grpc_server_started_total{namespace=~"rhacs-.*", job="central", grpc_type="unary"}[10m]))
          record: central:grpc_server_started:total:rate10m

        # HTTP
        - expr: |
            sum by (namespace, rhacs_instance_id, rhacs_org_id, rhacs_org_name, rhacs_cluster_name, rhacs_environment)
            (rate(http_incoming_requests_total{namespace=~"rhacs-.*", job="central", code!~"5.."}[10m]))
          record: central:http_incoming_requests:not_5xx:rate10m

        - expr: |
            sum by (namespace, rhacs_instance_id, rhacs_org_id, rhacs_org_name, rhacs_cluster_name, rhacs_environment)
            (rate(http_incoming_requests_total{namespace=~"rhacs-.*", job="central"}[10m]))
          record: central:http_incoming_requests:total:rate10m

        # This is a time series of 0s (down) and 1s (up).
        # Success rate above 65% is floored to 1.
        # Success rate below 65% is floored to 0.
        - expr: |
            floor(
              (
                central:grpc_server_handled:OK:rate10m
              + central:http_incoming_requests:not_5xx:rate10m
              ) / (
                central:grpc_server_started:total:rate10m
              + central:http_incoming_requests:total:rate10m
              )
              + 0.35
            )
          record: central:sli:error_rate

        # Availability SLI
        # If any of the SLIs is violated, the service counts as unavailable.
        # This is a time series of 0s (down) and 1s (up).
        - expr: |
            central:sli:pod_ready * on (namespace) group_right() central:sli:error_rate
          record: central:sli:availability

        - expr: |
            avg_over_time(central:sli:availability[1h])
          record: central:sli:availability:avg_over_time1h

        - expr: |
            avg_over_time(central:sli:availability[28d])
          record: central:sli:availability:avg_over_time28d

    - name: rhacs-central.slo
      rules:
        # Availability SLO
        - expr: "0.99"
          record: central:slo:availability

        # 0% exhaustion means no recorded failures.
        # 100% exhaustion means the SLO target has been reached.
        # >100% exhaustion means the SLO target has been violated.
        - expr: |
            (1 - central:sli:availability:avg_over_time28d) / (1 - scalar(central:slo:availability))
          record: central:slo:availability:error_budget_exhaustion

        # A burn rate of 1 corresponds to full error budget exhaustion after the SLO window W.
        # A burn rate of n corresponds to full error budget exhaustion after W/n - in other words,
        # it measures the exhaustion velocity. To keep the SLO target, a temporary burn rate larger
        # than 1 must be compensated with a burn rate smaller than 1.
        - expr: |
            (1 - central:sli:availability:avg_over_time1h) / (1 - scalar(central:slo:availability))
          record: central:slo:availability:burnrate1h

    - name: rhacs-central.alerts
      rules:
        - alert: Central availability error budget exhaustion - 90%
          annotations:
            message: "High availability error budget exhaustion for central. Current exhaustion: {{ $value | humanizePercentage }}."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-018-rhacs-central-slo-alerts.md"
          expr: |
            central:slo:availability:error_budget_exhaustion >= 0.9
          labels:
            service: central
            severity: critical
            team: appsre
            namespace: "{{ $labels.namespace }}"
            rhacs_instance_id: "{{ $labels.rhacs_instance_id }}"
            rhacs_org_name: "{{ $labels.rhacs_org_name }}"
            rhacs_org_id: "{{ $labels.rhacs_org_id }}"
            rhacs_cluster_name: "{{ $labels.rhacs_cluster_name }}"
            rhacs_environment: "{{ $labels.rhacs_environment }}"

        - alert: Central availability error budget exhaustion - 70%
          annotations:
            message: "High availability error budget exhaustion for central. Current exhaustion: {{ $value | humanizePercentage }}."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-018-rhacs-central-slo-alerts.md"
          expr: |
            central:slo:availability:error_budget_exhaustion >= 0.7
          labels:
            service: central
            severity: warning
            namespace: "{{ $labels.namespace }}"
            rhacs_instance_id: "{{ $labels.rhacs_instance_id }}"
            rhacs_org_name: "{{ $labels.rhacs_org_name }}"
            rhacs_org_id: "{{ $labels.rhacs_org_id }}"
            rhacs_cluster_name: "{{ $labels.rhacs_cluster_name }}"
            rhacs_environment: "{{ $labels.rhacs_environment }}"

        - alert: Central availability error budget exhaustion - 50%
          annotations:
            message: "High availability error budget exhaustion for central. Current exhaustion: {{ $value | humanizePercentage }}."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-018-rhacs-central-slo-alerts.md"
          expr: |
            central:slo:availability:error_budget_exhaustion >= 0.5
          labels:
            service: central
            severity: warning
            namespace: "{{ $labels.namespace }}"
            rhacs_instance_id: "{{ $labels.rhacs_instance_id }}"
            rhacs_org_name: "{{ $labels.rhacs_org_name }}"
            rhacs_org_id: "{{ $labels.rhacs_org_id }}"
            rhacs_cluster_name: "{{ $labels.rhacs_cluster_name }}"
            rhacs_environment: "{{ $labels.rhacs_environment }}"

        - alert: Central high availability burn rate
          annotations:
            message: "High availability burn rate for central. Current burn rate per hour: {{ $value | humanize }}."
            sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-managed-service-runbooks/blob/master/sops/dp-018-rhacs-central-slo-alerts.md"
          # Corresponds to less than 50% up time over 1 hour assuming a 99% SLO target.
          # See recording rules for how burn rates relate to SLI and SLO in general.
          expr: |
            central:slo:availability:burnrate1h >= 50
          labels:
            service: central
            severity: critical
            team: appsre
            namespace: "{{ $labels.namespace }}"
            rhacs_instance_id: "{{ $labels.rhacs_instance_id }}"
            rhacs_org_name: "{{ $labels.rhacs_org_name }}"
            rhacs_org_id: "{{ $labels.rhacs_org_id }}"
            rhacs_cluster_name: "{{ $labels.rhacs_cluster_name }}"
            rhacs_environment: "{{ $labels.rhacs_environment }}"
