rule_files:
  - /tmp/prometheus-rules-test.yaml

evaluation_interval: 1m

tests:
  - interval: 1m
    input_series:
      - series: kube_node_role{node="worker-1", role="acscs-worker"}
        values: "1"
      - series: kube_node_labels{node="worker-1", label_failure_domain_beta_kubernetes_io_zone="us-east-1a"}
        values: "1"
      - series: kube_node_status_allocatable{node="worker-1", resource="memory", job="kube-state-metrics"}
        values: "100"
      - series: cluster:namespace:pod_memory:active:kube_pod_container_resource_limits{node="worker-1", resource="memory", job="kube-state-metrics"}
        values: "201"
    alert_rule_test:
      - eval_time: 1m
        alertname: WorkerNodesMemoryOverCommit
        exp_alerts: []
      - eval_time: 5m
        alertname: WorkerNodesMemoryOverCommit
        exp_alerts:
          - exp_labels:
              alertname: WorkerNodesMemoryOverCommit
              severity: critical
            exp_annotations:
              description: "During the last 5 minutes, the average Memory limit commitment on worker nodes was 201%. This is above the recommended threshold of 200%."
              summary: "There is a high risk of over-committing Memory resources on worker nodes."
              sop_url: "https://gitlab.cee.redhat.com/stackrox/acs-cloud-service/runbooks/-/blob/master/sops/dp-027-cluster-scale-up.md"
